{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Description**\n",
        "\n",
        "This notebook implements an advanced portfolio optimization strategy using Deep Learning models (specifically a **Gated Recurrent Unit (GRU)** network) trained directly to maximize the **Sharpe Ratio**.\n",
        "\n",
        "The core part lies in the **hybrid feature set** and the **custom loss function**:\n",
        "1.  **Hybrid Feature Engineering:** The model input is a combination of:\n",
        "    * **Low-Rank Risk Factors:** Principal Components (PCA) extracted from the covariance matrix of returns to capture market-wide risk exposures.\n",
        "\n",
        "    * **Technical Indicators:** Standard momentum and volatility signals (RSI, MACD, etc.).\n",
        "\n",
        "2.  **Custom Sharpe Ratio Loss:** The model is trained using a custom loss function that minimizes the **Negative Sharpe Ratio** while including a **concentration penalty** (Herfindahl-Hirschman Index-inspired) to enforce diversification and prevent over-allocation to single assets.\n",
        "\n",
        "## **Methodology Stages**\n",
        "\n",
        "1.  **Data Acquisition:** Historical prices for a curated set of 87 European stocks are downloaded.\n",
        "\n",
        "2.  **Feature Engineering:** Technical indicators are calculated, and factors are extracted via PCA/SVD on the returns covariance matrix.\n",
        "\n",
        "3.  **Model Architecture:** A sequential GRU model and a Feedforward Neural Network (FNN) benchmark are trained to output long-only (Softmax-constrained) daily portfolio weights.\n",
        "\n",
        "4.  **Out-of-Sample Simulation:** The models' performance is evaluated on unseen test data (post-2024-01-01) based on key financial metrics.\n",
        "\n",
        "WARNING:\n",
        "\n",
        "This notebook is not a financial advisor.\n",
        "\n"
      ],
      "metadata": {
        "id": "W1MGP5mp1Dyc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install ta"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fDCVpYBXZUK1",
        "outputId": "14d959aa-16d6-43cc-bbc5-59c1b45690d5"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ta\n",
            "  Downloading ta-0.11.0.tar.gz (25 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from ta) (2.0.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from ta) (2.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->ta) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->ta) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->ta) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->ta) (1.17.0)\n",
            "Building wheels for collected packages: ta\n",
            "  Building wheel for ta (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ta: filename=ta-0.11.0-py3-none-any.whl size=29412 sha256=95e266d7bd5b7c20079e719d7fcfd36184443fd0b9bf2b844dad18572e8accfa\n",
            "  Stored in directory: /root/.cache/pip/wheels/5c/a1/5f/c6b85a7d9452057be4ce68a8e45d77ba34234a6d46581777c6\n",
            "Successfully built ta\n",
            "Installing collected packages: ta\n",
            "Successfully installed ta-0.11.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import yfinance as yf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import ta\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from datetime import datetime"
      ],
      "metadata": {
        "id": "BCAiSkmPeiws"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "TICKERS_BY_COUNTRY = {\n",
        "    'Germany': ['ALV.DE', 'DBK.DE', 'CBK.DE', 'HAG.DE', 'DB1.DE', 'FPE.DE', 'DHER.DE', 'MUV2.DE', 'VNA.DE', 'SDF.DE'],\n",
        "    'France': ['BNP.PA', 'ACA.PA', 'GLE.PA', 'CS.PA', 'OR.PA', 'ENGI.PA', 'SCR.PA', 'CA.PA', 'PUB.PA', 'SAN.PA'],\n",
        "    'UK': ['LLOY.L', 'BARC.L', 'HSBA.L', 'NWG.L', 'SSE.L', 'AV.L', 'PRU.L', 'LGEN.L', 'AHT.L', 'BP.L'],\n",
        "    'Spain': ['SAN.MC', 'BBVA.MC', 'CABK.MC', 'AMS.MC', 'MAP.MC', 'SAB.MC', 'ELE.MC', 'ENG.MC', 'IBE.MC', 'IAG.MC'],\n",
        "    'Italy': ['ISP.MI', 'UCG.MI', 'BAMI.MI', 'BMED.MI', 'FBK.MI', 'G.MI', 'AZM.MI', 'PST.MI', 'RACE.MI', 'IP.MI'],\n",
        "    'Netherlands': ['INGA.AS', 'ADYEN.AS', 'ABN.AS', 'WKL.AS', 'AD.AS', 'ASML.AS', 'HEIA.AS', 'TKWY.AS', 'KPN.AS'],\n",
        "    'Sweden': ['NDA-SE.ST', 'SEB-A.ST', 'SHB-A.ST', 'SWED-A.ST', 'GETI-B.ST', 'VOLV-B.ST', 'AZN.ST', 'TELIA.ST', 'ESSITY-B.ST', 'ERIC-B.ST'],\n",
        "    'Switzerland': ['UBSG.SW', 'VETN.SW', 'ZURN.SW', 'NESN.SW', 'SGSN.SW', 'CFR.SW', 'GIVN.SW', 'SREN.SW', 'NOVN.SW'],\n",
        "    'Belgium': ['KBC.BR', 'ABI.BR', 'UCB.BR', 'ACKB.BR', 'SOLB.BR', 'TUB.BR', 'ELI.BR', 'WDP.BR', 'COLR.BR']\n",
        "}\n",
        "\n",
        "TICKERS = [ticker for sublist in TICKERS_BY_COUNTRY.values() for ticker in sublist]\n",
        "START_DATE = '2021-01-01'\n",
        "END_DATE = '2025-09-30'\n",
        "NUM_ASSETS = len(TICKERS)\n",
        "\n",
        "SEQ_LENGTH = 10\n",
        "TRAIN_TEST_SPLIT_DATE = '2024-01-01'\n",
        "K_FACTORS = 5\n",
        "\n",
        "print(f\"Total number of assets: **{NUM_ASSETS}**\")\n",
        "\n",
        "\n",
        "def stage_1_acquire_and_prepare_data(tickers, start, end):\n",
        "\n",
        "    print(\"\\n--- Stage 1: Data Acquisition and Preparation ---\")\n",
        "\n",
        "\n",
        "    data = yf.download(tickers, start=start, end=end, progress=False)\n",
        "\n",
        "\n",
        "    close_prices_all = data['Close'].copy()\n",
        "\n",
        "\n",
        "    close_prices_filled = close_prices_all.ffill().bfill()\n",
        "\n",
        "\n",
        "    daily_returns_all = close_prices_filled.pct_change()\n",
        "\n",
        "\n",
        "    daily_returns_for_check = daily_returns_all.iloc[1:]\n",
        "    valid_tickers = daily_returns_for_check.columns[daily_returns_for_check.notna().all()].tolist()\n",
        "\n",
        "    if not valid_tickers:\n",
        "        print(\"CRITICAL ERROR: No assets with complete daily returns found after filling NaNs and checking for completeness.\")\n",
        "        print(\"This often happens if all chosen tickers were delisted, had extremely sparse data, or the date range is too strict.\")\n",
        "\n",
        "        return pd.DataFrame(), pd.DataFrame()\n",
        "\n",
        "\n",
        "    print(f\"Number of clean assets: **{len(valid_tickers)}** (from {len(tickers)} initial)\")\n",
        "\n",
        "\n",
        "    data_filtered_by_tickers = data.loc[:, (slice(None), valid_tickers)]\n",
        "\n",
        "\n",
        "    data_clean = data_filtered_by_tickers.dropna()\n",
        "\n",
        "\n",
        "    close_prices_clean = data_clean['Close']\n",
        "    daily_returns_clean = close_prices_clean.pct_change().dropna(how='all')\n",
        "\n",
        "\n",
        "    print(f\"Clean OHLCV Data shape: {data_clean.shape}\")\n",
        "    print(f\"Clean Daily Returns shape: {daily_returns_clean.shape}\")\n",
        "\n",
        "    return data_clean, daily_returns_clean\n",
        "\n",
        "full_price_data, daily_returns = stage_1_acquire_and_prepare_data(TICKERS, START_DATE, END_DATE)\n",
        "\n",
        "\n",
        "def stage_2_feature_engineering(full_price_data):\n",
        "\n",
        "    print(\"\\n--- Stage 2: Feature Engineering (Technical Indicators) ---\")\n",
        "\n",
        "    features = []\n",
        "\n",
        "    asset_names = full_price_data.columns.get_level_values(1).unique().tolist()\n",
        "\n",
        "\n",
        "    for ticker in asset_names:\n",
        "\n",
        "        df = full_price_data.loc[:, (slice(None), ticker)].copy()\n",
        "\n",
        "\n",
        "        df.columns = df.columns.droplevel(1)\n",
        "\n",
        "\n",
        "        df.loc[:, 'MA20'] = df['Close'].rolling(window=20).mean()\n",
        "        df.loc[:, 'EMA50'] = ta.trend.ema_indicator(df['Close'], window=50)\n",
        "        macd = ta.trend.MACD(df['Close'])\n",
        "        df.loc[:, 'MACD'] = macd.macd_diff()\n",
        "\n",
        "\n",
        "        df.loc[:, 'RSI'] = ta.momentum.rsi(df['Close'], window=14)\n",
        "\n",
        "        stoch = ta.momentum.StochasticOscillator(high=df['High'], low=df['Low'], close=df['Close'])\n",
        "        df.loc[:, 'STOCH_K'] = stoch.stoch()\n",
        "\n",
        "\n",
        "        bb = ta.volatility.BollingerBands(close=df['Close'], window=20)\n",
        "        df.loc[:, 'BB_WIDTH'] = bb.bollinger_wband()\n",
        "\n",
        "        df.loc[:, 'ATR'] = ta.volatility.AverageTrueRange(high=df['High'], low=df['Low'], close=df['Close'], window=14).average_true_range()\n",
        "\n",
        "\n",
        "\n",
        "        indicator_cols = ['MA20', 'EMA50', 'MACD', 'RSI', 'STOCH_K', 'BB_WIDTH', 'ATR']\n",
        "        asset_features = df[indicator_cols].copy()\n",
        "        asset_features.columns = [f'{ticker}_{col}' for col in indicator_cols]\n",
        "        features.append(asset_features)\n",
        "\n",
        "\n",
        "    feature_matrix = pd.concat(features, axis=1)\n",
        "\n",
        "\n",
        "    feature_matrix_clean = feature_matrix.dropna()\n",
        "    print(f\"Feature Matrix shape: {feature_matrix_clean.shape}\")\n",
        "    print(f\"Total features: **{len(feature_matrix_clean.columns)}** (7 indicators * {len(asset_names)} assets)\")\n",
        "\n",
        "    return feature_matrix_clean\n",
        "\n",
        "feature_matrix = stage_2_feature_engineering(full_price_data)\n",
        "\n",
        "\n",
        "def stage_3_low_rank_feature_extraction(returns_df, variance_threshold=0.90):\n",
        "\n",
        "    print(\"\\n--- Stage 3: Low-Rank Feature Extraction (Compression) ---\")\n",
        "\n",
        "\n",
        "    aligned_returns = returns_df.loc[feature_matrix.index]\n",
        "\n",
        "\n",
        "    covariance_matrix = aligned_returns.cov()\n",
        "    print(f\"Covariance Matrix shape: {covariance_matrix.shape}\")\n",
        "\n",
        "\n",
        "    eigen_values, eigen_vectors = np.linalg.eigh(covariance_matrix.values)\n",
        "\n",
        "\n",
        "    sorted_indices = np.argsort(eigen_values)[::-1]\n",
        "    eigen_values = eigen_values[sorted_indices]\n",
        "    eigen_vectors = eigen_vectors[:, sorted_indices]\n",
        "\n",
        "\n",
        "    total_variance = np.sum(eigen_values)\n",
        "    cumulative_variance_ratio = np.cumsum(eigen_values) / total_variance\n",
        "\n",
        "\n",
        "    k_factors = np.where(cumulative_variance_ratio >= variance_threshold)[0][0] + 1\n",
        "\n",
        "    print(f\"Total Variance: {total_variance:.4f}\")\n",
        "    print(f\"Number of factors (k) retaining >= {variance_threshold*100}% variance: **{k_factors}**\")\n",
        "\n",
        "\n",
        "    factor_vectors = eigen_vectors[:, :k_factors]\n",
        "\n",
        "\n",
        "    returns_values = aligned_returns.values\n",
        "    factor_returns_values = returns_values @ factor_vectors\n",
        "\n",
        "\n",
        "    factor_returns = pd.DataFrame(\n",
        "        factor_returns_values,\n",
        "        index=aligned_returns.index,\n",
        "        columns=[f'Factor_{i+1}' for i in range(k_factors)]\n",
        "    )\n",
        "\n",
        "    print(f\"Factor Returns shape: {factor_returns.shape}\")\n",
        "\n",
        "\n",
        "    final_input_df = pd.concat([factor_returns, feature_matrix.loc[factor_returns.index]], axis=1)\n",
        "\n",
        "    return final_input_df, aligned_returns, k_factors\n",
        "\n",
        "final_input_data, aligned_returns, K_FACTORS = stage_3_low_rank_feature_extraction(daily_returns)\n",
        "\n",
        "\n",
        "INPUT_SIZE = final_input_data.shape[1]\n",
        "HIDDEN_SIZE_RNN = 64\n",
        "NUM_LAYERS_RNN = 2\n",
        "NUM_ASSETS_FINAL = aligned_returns.shape[1]\n",
        "\n",
        "\n",
        "class SharpeRatioLoss(nn.Module):\n",
        "\n",
        "    def __init__(self, risk_free_rate=0.0, concentration_lambda=0.1):\n",
        "        super().__init__()\n",
        "        self.risk_free_rate = risk_free_rate\n",
        "        self.concentration_lambda = concentration_lambda\n",
        "\n",
        "    def forward(self, weights, returns):\n",
        "\n",
        "        portfolio_returns = torch.sum(weights * returns, dim=1)\n",
        "\n",
        "\n",
        "        mean_return = torch.mean(portfolio_returns)\n",
        "        std_dev = torch.std(portfolio_returns)\n",
        "\n",
        "\n",
        "        epsilon = 1e-6\n",
        "\n",
        "\n",
        "        sharpe_ratio = (mean_return - self.risk_free_rate) / (std_dev + epsilon)\n",
        "        negative_sharpe_ratio = -sharpe_ratio\n",
        "\n",
        "\n",
        "        max_weight = torch.max(weights, dim=1).values\n",
        "        penalty = torch.relu(max_weight - 0.50)\n",
        "        concentration_penalty = torch.mean(penalty) * 100\n",
        "\n",
        "        total_loss = negative_sharpe_ratio + self.concentration_lambda * concentration_penalty\n",
        "\n",
        "        return total_loss\n",
        "\n",
        "\n",
        "class PortfolioGRU(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
        "        super().__init__()\n",
        "\n",
        "        self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first=True)\n",
        "\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        gru_out, _ = self.gru(x)\n",
        "\n",
        "        last_step_out = gru_out[:, -1, :]\n",
        "        raw_weights = self.fc(last_step_out)\n",
        "\n",
        "        weights = self.softmax(raw_weights)\n",
        "        return weights\n",
        "\n",
        "\n",
        "class PortfolioFNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super().__init__()\n",
        "\n",
        "        self.layer1 = nn.Linear(input_size * SEQ_LENGTH, hidden_size)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.layer2 = nn.Linear(hidden_size, hidden_size)\n",
        "        self.fc_out = nn.Linear(hidden_size, output_size)\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        x = x.reshape(x.size(0), -1)\n",
        "\n",
        "        x = self.relu(self.layer1(x))\n",
        "        x = self.relu(self.layer2(x))\n",
        "        raw_weights = self.fc_out(x)\n",
        "        weights = self.softmax(raw_weights)\n",
        "        return weights\n",
        "\n",
        "def prepare_dl_data(data, returns, seq_length, train_split_date):\n",
        "\n",
        "\n",
        "\n",
        "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "    scaled_data = scaler.fit_transform(data)\n",
        "    scaled_df = pd.DataFrame(scaled_data, index=data.index, columns=data.columns)\n",
        "\n",
        "    X, Y = [], []\n",
        "    for i in range(seq_length, len(scaled_df)):\n",
        "\n",
        "        X.append(scaled_df.iloc[i-seq_length:i].values)\n",
        "\n",
        "        Y.append(returns.iloc[i].values)\n",
        "\n",
        "    X = np.array(X)\n",
        "    Y = np.array(Y)\n",
        "\n",
        "\n",
        "    dates = scaled_df.index[seq_length:]\n",
        "\n",
        "\n",
        "    train_idx = dates < train_split_date\n",
        "    test_idx = dates >= train_split_date\n",
        "\n",
        "    X_train, Y_train = X[train_idx], Y[train_idx]\n",
        "    X_test, Y_test = X[test_idx], Y[test_idx]\n",
        "\n",
        "\n",
        "    X_train_t = torch.tensor(X_train, dtype=torch.float32)\n",
        "    Y_train_t = torch.tensor(Y_train, dtype=torch.float32)\n",
        "    X_test_t = torch.tensor(X_test, dtype=torch.float32)\n",
        "    Y_test_t = torch.tensor(Y_test, dtype=torch.float32)\n",
        "\n",
        "    print(f\"\\nTraining set size: {X_train_t.shape[0]} days\")\n",
        "    print(f\"Testing set size: {X_test_t.shape[0]} days\")\n",
        "\n",
        "\n",
        "    test_returns = returns.loc[dates[test_idx]]\n",
        "\n",
        "    return X_train_t, Y_train_t, X_test_t, Y_test_t, test_returns\n",
        "\n",
        "X_train, Y_train, X_test, Y_test, test_returns_df = prepare_dl_data(\n",
        "    final_input_data, aligned_returns, SEQ_LENGTH, TRAIN_TEST_SPLIT_DATE\n",
        ")\n",
        "\n",
        "\n",
        "def train_model(model, X_train, Y_train, epochs=200, lr=0.001):\n",
        "\n",
        "    criterion = SharpeRatioLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "\n",
        "\n",
        "        weights = model(X_train)\n",
        "\n",
        "        loss = criterion(weights, Y_train)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if (epoch + 1) % 50 == 0:\n",
        "            print(f'Epoch [{epoch+1}/{epochs}], Loss (Neg Sharpe): {loss.item():.4f}')\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "print(\"\\n--- Training Model 1: GRU-based Optimizer ---\")\n",
        "gru_model = PortfolioGRU(INPUT_SIZE, HIDDEN_SIZE_RNN, NUM_LAYERS_RNN, NUM_ASSETS_FINAL)\n",
        "gru_model = train_model(gru_model, X_train, Y_train, epochs=200)\n",
        "\n",
        "print(\"\\n--- Training Model 2: FNN Benchmarking Model ---\")\n",
        "fnn_model = PortfolioFNN(INPUT_SIZE, HIDDEN_SIZE_RNN, NUM_ASSETS_FINAL)\n",
        "fnn_model = train_model(fnn_model, X_train, Y_train, epochs=200)\n",
        "\n",
        "\n",
        "def simulate_portfolio(model, X_test, returns_df):\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "\n",
        "        weights_t = model(X_test).numpy()\n",
        "\n",
        "\n",
        "    portfolio_returns = np.sum(weights_t * returns_df.values, axis=1)\n",
        "\n",
        "\n",
        "    portfolio_returns_series = pd.Series(portfolio_returns, index=returns_df.index)\n",
        "\n",
        "    return portfolio_returns_series, weights_t\n",
        "\n",
        "\n",
        "gru_portfolio_returns, gru_weights = simulate_portfolio(gru_model, X_test, test_returns_df)\n",
        "fnn_portfolio_returns, fnn_weights = simulate_portfolio(fnn_model, X_test, test_returns_df)\n",
        "\n",
        "\n",
        "print(\"\\n--- Stage 5: Portfolio Simulation Complete ---\")\n",
        "print(f\"GRU Portfolio Returns simulated over {len(gru_portfolio_returns)} days.\")\n",
        "\n",
        "def calculate_metrics(returns_series, weights_array=None, name=\"Portfolio\"):\n",
        "\n",
        "\n",
        "\n",
        "    annual_returns = returns_series.mean() * 252\n",
        "    annual_volatility = returns_series.std() * np.sqrt(252)\n",
        "    sharpe_ratio = annual_returns / annual_volatility\n",
        "\n",
        "    cumulative_return = (1 + returns_series).prod() - 1\n",
        "\n",
        "\n",
        "    cumulative_wealth = (1 + returns_series).cumprod()\n",
        "    peak = cumulative_wealth.expanding(min_periods=1).max()\n",
        "    drawdown = (cumulative_wealth / peak) - 1\n",
        "    max_drawdown = drawdown.min()\n",
        "\n",
        "    metrics = {\n",
        "        'Name': name,\n",
        "        'Sharpe Ratio': sharpe_ratio,\n",
        "        'Annual Volatility ($\\sigma_p$)': annual_volatility,\n",
        "        'Cumulative Return': cumulative_return,\n",
        "        'Maximum Drawdown (MDD)': max_drawdown\n",
        "    }\n",
        "\n",
        "\n",
        "    if weights_array is not None:\n",
        "\n",
        "        daily_hhi = np.sum(weights_array**2, axis=1)\n",
        "        mean_hhi = np.mean(daily_hhi)\n",
        "\n",
        "\n",
        "        mean_ena = 1 / mean_hhi\n",
        "\n",
        "        metrics['Mean HHI'] = mean_hhi\n",
        "        metrics['Mean ENA'] = mean_ena\n",
        "\n",
        "    return metrics\n",
        "\n",
        "\n",
        "gru_metrics = calculate_metrics(gru_portfolio_returns, gru_weights, \"GRU Model Portfolio\")\n",
        "fnn_metrics = calculate_metrics(fnn_portfolio_returns, fnn_weights, \"FNN Model Portfolio\")\n",
        "\n",
        "\n",
        "\n",
        "report_df = pd.DataFrame([gru_metrics, fnn_metrics])\n",
        "\n",
        "\n",
        "report_df['Sharpe Ratio'] = report_df['Sharpe Ratio'].round(4)\n",
        "report_df['Annual Volatility ($\\sigma_p$)'] = (report_df['Annual Volatility ($\\sigma_p$)'] * 100).round(2).astype(str) + '%'\n",
        "report_df['Cumulative Return'] = (report_df['Cumulative Return'] * 100).round(2).astype(str) + '%'\n",
        "report_df['Maximum Drawdown (MDD)'] = (report_df['Maximum Drawdown (MDD)'].abs() * 100).round(2).astype(str) + '%'\n",
        "if 'Mean HHI' in report_df.columns:\n",
        "    report_df['Mean HHI'] = report_df['Mean HHI'].round(4)\n",
        "if 'Mean ENA' in report_df.columns:\n",
        "    report_df['Mean ENA'] = report_df['Mean ENA'].round(2)\n",
        "\n",
        "\n",
        "print(\"\\n--- Stage 6: Performance Report (Out-of-Sample) ---\")\n",
        "print(report_df.to_markdown(index=False))\n",
        "\n",
        "\n",
        "best_model_name = report_df.iloc[report_df['Sharpe Ratio'].idxmax()]['Name']\n",
        "print(f\"\\n **Best Performing Model (Highest Sharpe Ratio): {best_model_name}**\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W94JPm5UeizK",
        "outputId": "b0edd517-0a74-4ddd-c16c-918dd8ee47a2"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<>:396: SyntaxWarning: invalid escape sequence '\\s'\n",
            "<>:425: SyntaxWarning: invalid escape sequence '\\s'\n",
            "<>:425: SyntaxWarning: invalid escape sequence '\\s'\n",
            "<>:396: SyntaxWarning: invalid escape sequence '\\s'\n",
            "<>:425: SyntaxWarning: invalid escape sequence '\\s'\n",
            "<>:425: SyntaxWarning: invalid escape sequence '\\s'\n",
            "/tmp/ipython-input-3032880459.py:396: SyntaxWarning: invalid escape sequence '\\s'\n",
            "  'Annual Volatility ($\\sigma_p$)': annual_volatility,\n",
            "/tmp/ipython-input-3032880459.py:425: SyntaxWarning: invalid escape sequence '\\s'\n",
            "  report_df['Annual Volatility ($\\sigma_p$)'] = (report_df['Annual Volatility ($\\sigma_p$)'] * 100).round(2).astype(str) + '%'\n",
            "/tmp/ipython-input-3032880459.py:425: SyntaxWarning: invalid escape sequence '\\s'\n",
            "  report_df['Annual Volatility ($\\sigma_p$)'] = (report_df['Annual Volatility ($\\sigma_p$)'] * 100).round(2).astype(str) + '%'\n",
            "/tmp/ipython-input-3032880459.py:31: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  data = yf.download(tickers, start=start, end=end, progress=False)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of assets: **87**\n",
            "\n",
            "--- Stage 1: Data Acquisition and Preparation ---\n",
            "Number of clean assets: **87** (from 87 initial)\n",
            "Clean OHLCV Data shape: (1158, 435)\n",
            "Clean Daily Returns shape: (1157, 87)\n",
            "\n",
            "--- Stage 2: Feature Engineering (Technical Indicators) ---\n",
            "Feature Matrix shape: (1109, 609)\n",
            "Total features: **609** (7 indicators * 87 assets)\n",
            "\n",
            "--- Stage 3: Low-Rank Feature Extraction (Compression) ---\n",
            "Covariance Matrix shape: (87, 87)\n",
            "Total Variance: 0.0303\n",
            "Number of factors (k) retaining >= 90.0% variance: **45**\n",
            "Factor Returns shape: (1109, 45)\n",
            "\n",
            "Training set size: 677 days\n",
            "Testing set size: 422 days\n",
            "\n",
            "--- Training Model 1: GRU-based Optimizer ---\n",
            "Epoch [50/200], Loss (Neg Sharpe): -0.3271\n",
            "Epoch [100/200], Loss (Neg Sharpe): -0.5326\n",
            "Epoch [150/200], Loss (Neg Sharpe): -0.7390\n",
            "Epoch [200/200], Loss (Neg Sharpe): -0.8805\n",
            "\n",
            "--- Training Model 2: FNN Benchmarking Model ---\n",
            "Epoch [50/200], Loss (Neg Sharpe): -0.1517\n",
            "Epoch [100/200], Loss (Neg Sharpe): -0.2383\n",
            "Epoch [150/200], Loss (Neg Sharpe): -0.3464\n",
            "Epoch [200/200], Loss (Neg Sharpe): -0.4490\n",
            "\n",
            "--- Stage 5: Portfolio Simulation Complete ---\n",
            "GRU Portfolio Returns simulated over 422 days.\n",
            "\n",
            "--- Stage 6: Performance Report (Out-of-Sample) ---\n",
            "| Name                |   Sharpe Ratio | Annual Volatility ($\\sigma_p$)   | Cumulative Return   | Maximum Drawdown (MDD)   |   Mean HHI |   Mean ENA |\n",
            "|:--------------------|---------------:|:---------------------------------|:--------------------|:-------------------------|-----------:|-----------:|\n",
            "| GRU Model Portfolio |         2.0118 | 15.35%                           | 64.4%               | 12.49%                   |     0.2059 |       4.86 |\n",
            "| FNN Model Portfolio |         1.208  | 15.91%                           | 35.06%              | 17.48%                   |     0.2568 |       3.89 |\n",
            "\n",
            " **Best Performing Model (Highest Sharpe Ratio): GRU Model Portfolio**\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Results**\n",
        "\n",
        "## **Key Results**\n",
        "\n",
        "The custom SharpeRatioLoss function included an explicit concentration penalty ($L_{concentration}$):$$\\text{penalty} = \\text{torch.relu}(\\max(\\text{weights}) - 0.50)$$This term added to the total loss if any single asset's weight exceeded 50%.\n",
        "\n",
        "\n",
        "By minimizing this loss, the model was actively discouraged from heavily concentrating in one or two assets.\n",
        "\n",
        "The **GRU Model Portfolio** demonstrated superior performance on the out-of-sample test set:\n",
        "\n",
        "| Metric | GRU Model Portfolio | FNN Model Portfolio |\n",
        "| :--- | :--- | :--- |\n",
        "| **Sharpe Ratio** | **2.0118 (Outstanding)** | 1.2080 |\n",
        "| Cumulative Return | **64.40%** | 35.06% |\n",
        "| Max Drawdown (MDD) | **12.49%** | 17.48% |\n",
        "| Mean ENA (Diversification) | **4.86** | 3.89 |\n",
        "\n",
        "**WARNING**:\n",
        "\n",
        "This notebook is not a financial advisor.\n",
        "\n"
      ],
      "metadata": {
        "id": "1SAPquJW1kkF"
      }
    }
  ]
}